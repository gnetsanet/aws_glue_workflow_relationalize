{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1602238379821_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-72-24-88.ec2.internal:20888/proxy/application_1602238379821_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-72-17-142.ec2.internal:8042/node/containerlogs/container_1602238379821_0004_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a76b534b7de4cb69f621da337241541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd731190f7894b9b97af459470988c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import ResolveChoice,Relationalize\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "## @params: [JOB_NAME]\n",
    "## args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "## sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "## job.init(args['JOB_NAME'], args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232ffb9873454a47bf6a367a06519dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glue_db_name = 'nyphil_db'\n",
    "glue_orig_table_name = 'ny_phil_perf_hist'\n",
    "s3_temp_folder = 's3://bdb683-databucket-1uqqkb2fs03br/tmp/'\n",
    "s3_target_path = 's3://bdb683-databucket-1uqqkb2fs03br/data-lake2/'\n",
    "redshift_db_name = 'nyphil_db'\n",
    "redshift_connection = 'ny_phil_redshift_conn'\n",
    "redshift_schema = 'ny_phil'\n",
    "root_table = 'ny_phil_perf_hist'\n",
    "lvl_to_denorm = 99\n",
    "num_output_files = 4\n",
    "keep_table_prefix = False\n",
    "target_repo = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757787a2c3134ffbbba80a35018fafb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @type: DataSource\n",
    "# @args: [database = glue_db_name, table_name = glue_orig_table_name, transformation_ctx = \"datasource0\"]\n",
    "# @return: datasource0\n",
    "# @inputs: []\n",
    "datasource0 = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database=glue_db_name, table_name=glue_orig_table_name, transformation_ctx=\"datasource0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource0.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e91fc84afe4ba882be55434a839dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resolvechoice2 = ResolveChoice.apply(\n",
    "    frame=datasource0, choice=\"make_struct\", transformation_ctx=\"resolvechoice2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolvechoice2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044259fb5f1441bebbeb47ecdf4ce679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relationalize4 = Relationalize.apply(\n",
    "    frame=resolvechoice2, staging_path=s3_temp_folder, name=root_table, transformation_ctx=\"relationalize4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name in relationalize4.keys():\n",
    "    print(df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name in relationalize4.keys():\n",
    "    print(df_name)\n",
    "    relationalize4.select(df_name).toDF().schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationalize4.select('ny_phil_perf_hist_programs').toDF().schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598a7ab65ee34b51982371a86ff4e78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dynframes_list = {}\n",
    "dataframes_list = {}\n",
    "s3_target_path_list = {}\n",
    "column_names_list = {}\n",
    "table_names_list = []\n",
    "datasink = {}\n",
    "\n",
    "# variables needed to automatically join the tables\n",
    "# lvl_to_denorm is set to 0 by default, this means no join will be processed\n",
    "# set it to 99 to create a single denormalized table, or to an integer corrisponding\n",
    "# to the nested levels you want to join (if there are less levels than requested a single table will be created)\n",
    "\n",
    "join_lvl = 0\n",
    "join_start_lvl = 0\n",
    "table_tojoin = {}\n",
    "denorm_df_list={}\n",
    "# set the correct level to start the denormalization based on the input parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7975797669b14c3180871529fb305443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# helper functions definition, this are basic snippet of codes used to simplify and standardize\n",
    "# the name cleansing and writing to the target data stores:\n",
    "\n",
    "# add a prefix to a name\n",
    "\n",
    "def add_prefix(c, prefix):\n",
    "    if prefix in c:\n",
    "        r = c\n",
    "    else:\n",
    "        r = prefix+'_'+c\n",
    "    return r\n",
    "\n",
    "# check if column is a foreign key \n",
    "\n",
    "def is_foreign_key(name):\n",
    "    return name[-3:]==\"_fk\"\n",
    "\n",
    "# extract a table name from a foreign key (fk) attribute\n",
    "\n",
    "def get_child_tbl_name(name):\n",
    "    return name.replace(\"_fk\", \"\")\n",
    "\n",
    "# check if we want to denormalize the tables or not\n",
    "\n",
    "def is_to_denormalize(lvl_to_denorm):\n",
    "    return lvl_to_denorm>0\n",
    "\n",
    "# cleans a name from the automatic code inserted strings to make the table and columns name more readable\n",
    "\n",
    "def clean_name(name_type, tbl_name, col_name, reference_list, text_to_replace):\n",
    "    if name_type == 'table':\n",
    "        new_name = tbl_name.lower().replace(\n",
    "            text_to_replace, \"\").replace(\".\", \"_\").replace(\"_val_\", \"_\")\n",
    "    elif name_type == 'column':\n",
    "        new_name = col_name.lower().replace(\n",
    "            \".val.\", \"_\").replace(\".\", \"_\").replace(\"__\", \"_\")\n",
    "        if new_name in reference_list:\n",
    "            new_name = new_name+\"_fk\"\n",
    "        elif new_name == 'id':\n",
    "            new_name = tbl_name+\"_sk\"\n",
    "        elif new_name == 'index':\n",
    "            new_name = \"rownum\"\n",
    "        elif not keep_table_prefix:\n",
    "            new_name = new_name.replace(tbl_name+\"_\", \"\")\n",
    "    return new_name\n",
    "\n",
    "# find if the table is to be joined and which level in the join hierarchy is at\n",
    "\n",
    "def find_tbl_join_lvl(dict, tbl):\n",
    "    for k in dict:\n",
    "        if tbl in dict[k]:\n",
    "            lvl=int(k)\n",
    "            found=1\n",
    "            break\n",
    "        else:\n",
    "            lvl=0\n",
    "            found=0\n",
    "    return {'join_lvl':lvl,'table_found':found}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8176a592ed24718b2f08a73e04318b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for df_name in relationalize4.keys():\n",
    "    \n",
    "    if (df_name == root_table):\n",
    "        if len(relationalize4.select(root_table).toDF().schema.names) == 1:\n",
    "            tbl_name = relationalize4.select(root_table).toDF().schema.names[0]\n",
    "        else:\n",
    "             tbl_name = root_table\n",
    "        table_tojoin={'0': {tbl_name: {'join_to_tbls':[],'join_col':[]}}}\n",
    "        table_names_list.insert(0,tbl_name)\n",
    "    else:\n",
    "        tbl_name = clean_name('table', df_name, '', [], root_table+\"_\")\n",
    "        if tbl_name not in table_names_list:\n",
    "            table_names_list.append(tbl_name)\n",
    "    \n",
    "            \n",
    "    # create a list of S3 output bucket indexed by table name\n",
    "    s3_target_path_list[tbl_name] = s3_target_path+tbl_name\n",
    "    # create a list of dataframes indexed by table name\n",
    "    dataframes_list[tbl_name] = relationalize4.select(df_name).toDF()\n",
    "    # create a list of columns indexed by table name\n",
    "    column_names_list[tbl_name] = dataframes_list[tbl_name].schema.names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debe8fda6a114858ac05b71323ad7be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# write to target function, leverages glue native dynamic frame writer and standardize the parameter to pass,\n",
    "# added to avoid to repeate the code multiple time and assure it is always using consistent parameters\n",
    "\n",
    "\n",
    "def write_to_targets(tbl_name, dyn_frame, target_path, num_output_files, target_repo):\n",
    "\n",
    "    dyn_frame = dyn_frame.coalesce(num_output_files)\n",
    "    if target_repo == 's3' or target_repo == 'all':\n",
    "        datasink[tbl_name] = glueContext.write_dynamic_frame.from_options(frame=dyn_frame, connection_type=\"s3\",\n",
    "                                                                          connection_options={\n",
    "                                                                              \"path\": target_path},\n",
    "                                                                          format=\"glueparquet\",\n",
    "                                                                          transformation_ctx=\"datasink['tbl_name']\")\n",
    "    if target_repo == 'redshift' or target_repo == 'all':\n",
    "        db_tbl_name = redshift_schema+\".\"+tbl_name\n",
    "        glueContext.write_dynamic_frame.from_jdbc_conf(frame=dyn_frame,\n",
    "                                                       catalog_connection=redshift_connection,\n",
    "                                                       connection_options={\n",
    "                                                           \"dbtable\": db_tbl_name, \"database\": redshift_db_name},\n",
    "                                                       redshift_tmp_dir=s3_temp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20b364fa4024da98a49442e0d396b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop on the list of tables and for each table on the attributes to standardize each column name\n",
    "\n",
    "for tbl_name in table_names_list:\n",
    "    \n",
    "    # set the join_lvl for this table and if the table is in the list to be denormalized\n",
    "    join_lvl = find_tbl_join_lvl(table_tojoin, tbl_name)['join_lvl']\n",
    "    table_to_denorm=find_tbl_join_lvl(table_tojoin, tbl_name)['table_found']   \n",
    "    for col_name in column_names_list[tbl_name]:\n",
    "        \n",
    "        # Clean column name and rename the dataframe column \n",
    "        new_col_name = clean_name('column', tbl_name, col_name, table_names_list, '')\n",
    "        dataframes_list[tbl_name] = dataframes_list[tbl_name].withColumnRenamed(col_name, new_col_name).drop('rownum')\n",
    "        \n",
    "        # if the column is a foreing key and we have configured the job to run the normalization we will add the child table to the list of table to join \n",
    "        if is_foreign_key(new_col_name) and is_to_denormalize(lvl_to_denorm):\n",
    "            \n",
    "            # get the child table name and join level\n",
    "            child_tbl_name= get_child_tbl_name(new_col_name)\n",
    "            next_join_lvl= join_lvl+1\n",
    "            \n",
    "            # add the child table to the list of tables to join to for the current teable, and the foreign keys\n",
    "            table_tojoin[str(join_lvl)][tbl_name]['join_to_tbls'].append(child_tbl_name)\n",
    "            table_tojoin[str(join_lvl)][tbl_name]['join_col'].append(new_col_name)\n",
    "            table_tojoin[str(join_lvl)][tbl_name]['has_child']=True\n",
    "            \n",
    "            # add the child table to the next level in the table_tojoin dictionary\n",
    "            if len(table_tojoin) == next_join_lvl:\n",
    "                table_tojoin[str(next_join_lvl)]={}\n",
    "            table_tojoin[str(next_join_lvl)][child_tbl_name]={'join_to_tbls':[],'join_col':[],'has_child':False}\n",
    "                \n",
    "    # if the table is not in the list to be denormalized it does not have childs and depending the level of normalization requested it might be needed to write out anyway\n",
    "    if table_to_denorm == 0 and is_to_denormalize(lvl_to_denorm):\n",
    "        table_to_write_list.append(tbl_name)\n",
    "        \n",
    "    # convert to Dynamicframes in preparation for write\n",
    "    dynframes_list[tbl_name] = DynamicFrame.fromDF(\n",
    "        dataframes_list[tbl_name], glueContext, \"dynframes_list[tbl_name]\")\n",
    "    \n",
    "    # if no denormalizarion required write out the data\n",
    "    #if not is_to_denormalize(lvl_to_denorm):\n",
    "    #    write_to_targets(tbl, dynframes_list[tbl], s3_target_path_list[tbl],num_output_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932184093c0b468a886a4d3f5355c8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# recurse on the list of table to be joined and join starting by the deepest level\n",
    "def denormalize_table(table_tojoin,join_start_lvl, join_lvl,dataframes_list,denorm_df_list ):\n",
    "    # set the next join level and start looping on table to be joined at curent level (it will be the left table in the join)\n",
    "    next_join_level = join_start_lvl+1\n",
    "    for left_table in table_tojoin[str(join_start_lvl)]:\n",
    "        # last join level contains table with not further joins, so we check if we are already at the brfore last join level, if not we recurse to the next level \n",
    "        if next_join_level < join_lvl:\n",
    "            denormalize_table(table_tojoin,next_join_level, join_lvl,dataframes_list,denorm_df_list )\n",
    "        # if done with recursion execute the join, first set the left data frame for the join\n",
    "        df_left=dataframes_list[left_table]\n",
    "        \n",
    "        # loop on all table listed in the join_to_table list for the left_table if the table has childs\n",
    "        if table_tojoin[str(join_start_lvl)][left_table]['has_child']:\n",
    "            for right_table in table_tojoin[str(join_start_lvl)][left_table]['join_to_tbls']:\n",
    "            \n",
    "                # look up the foreing key and set it as join column for left table\n",
    "                join_col_index = table_tojoin[str(join_start_lvl)][left_table]['join_col'].index(right_table+\"_fk\")\n",
    "                join_col_left = table_tojoin[str(join_start_lvl)][left_table]['join_col'][join_col_index]\n",
    "            \n",
    "                # set the join column for the right table replacing the suffix (remember we have named the column in a prebious step so we own the naming convention)\n",
    "                join_col_right = join_col_left.replace(\"_fk\",\"_sk\")\n",
    "            \n",
    "                # if the right table had been already denormalized use that dataframe otherwise use the original dataframe\n",
    "                if right_table in denorm_df_list:\n",
    "                    df_right=denorm_df_list[right_table]\n",
    "                else:\n",
    "                    df_right=dataframes_list[right_table]\n",
    "            \n",
    "                # in case we decided to remove the table name prefix from the columns than check if there is a naming conflict and resolve it adding back the tablename prefix\n",
    "                left_cols = df_left.schema.names\n",
    "                right_cols = df_right.schema.names\n",
    "                #right_cols.pop(right_cols.index(\"rownum\"))\n",
    "                for col in right_cols:\n",
    "                    if col in left_cols:\n",
    "                        df_right=df_right.withColumnRenamed(col, right_table+\"_\"+col)\n",
    "                        right_cols.append(right_table+\"_\"+col)\n",
    "                        right_cols.remove(col)\n",
    "            \n",
    "                #execute the join matching left and right join column names to avoid column duplication in the output dataframe\n",
    "                df_left = df_left.join(df_right.withColumnRenamed(join_col_right, join_col_left), join_col_left, how='left_outer')\n",
    "        \n",
    "        #add the denormalized data frame and return it to the previous recursion level\n",
    "        denorm_df_list[left_table]=df_left\n",
    "            \n",
    "    return denorm_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25db94976824943baf192f5f14fe8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- programs_works_fk: long\n",
      "|-- programs_concerts_fk: long\n",
      "|-- programs_sk: long\n",
      "|-- id: string\n",
      "|-- programid: string\n",
      "|-- orchestra: string\n",
      "|-- season: string\n",
      "|-- eventtype: string\n",
      "|-- location: string\n",
      "|-- venue: string\n",
      "|-- date: string\n",
      "|-- time: string\n",
      "|-- programs_works_soloists_fk: long\n",
      "|-- programs_works_id: string\n",
      "|-- composername: string\n",
      "|-- conductorname: string\n",
      "|-- worktitle_string: string\n",
      "|-- worktitle_struct_: string\n",
      "|-- worktitle_struct_em: string\n",
      "|-- interval: string\n",
      "|-- movement_string: string\n",
      "|-- movement_struct_: string\n",
      "|-- movement_struct_em: string\n",
      "|-- soloistname: string\n",
      "|-- soloistinstrument: string\n",
      "|-- soloistroles: string"
     ]
    }
   ],
   "source": [
    "# if no denormalization needed we are done \n",
    "if not is_to_denormalize(lvl_to_denorm):\n",
    "    print('Job completed Successfully')\n",
    "else:\n",
    "    if lvl_to_denorm > join_lvl:\n",
    "        join_start_lvl = 0\n",
    "    else:\n",
    "        join_start_lvl = (join_lvl - lvl_to_denorm)\n",
    "        print('start denormalizing table(s)')\n",
    "\n",
    "    # start denormalization\n",
    "    denorm_df_list=denormalize_table(table_tojoin,join_start_lvl,join_lvl,dataframes_list,denorm_df_list)\n",
    "    \n",
    "    # loop to write out the right dynamic frames\n",
    "    for tbl in dynframes_list:\n",
    "        # we need to write only the denormalized dynamic frames and any frame that has been skipped by the process\n",
    "        table_to_be_joined_with_level = find_tbl_join_lvl(table_tojoin,tbl)\n",
    "        join_lvl_pr = find_tbl_join_lvl(table_tojoin, tbl)['join_lvl']\n",
    "        table_to_denorm_pr=find_tbl_join_lvl(table_tojoin, tbl)['table_found']\n",
    "        \n",
    "        if table_to_be_joined_with_level['join_lvl'] <= join_start_lvl:\n",
    "            if table_to_be_joined_with_level['table_found'] == 1 and table_to_be_joined_with_level['join_lvl'] == join_start_lvl:\n",
    "                # if a table has been normalized convert the dataframe to dynamicFrame \n",
    "                dynframes_list[tbl] = DynamicFrame.fromDF(denorm_df_list[tbl], glueContext, \"dynframes_list[tbl]\")\n",
    "            #write_to_targets(tbl, dynframes_list[tbl], s3_target_path_list[tbl],num_output_files)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
